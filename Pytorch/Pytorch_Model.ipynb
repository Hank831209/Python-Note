{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be8f9ce-a779-476c-84b0-7f4bd5a5863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73684c89-c1c1-4e01-87e2-be0facc85583",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "## Computation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f186e7a8-6951-4e6a-a4d3-d3509ce4eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor(3., grad_fn=<AddBackward0>)\n",
      "d tensor(2., grad_fn=<AddBackward0>)\n",
      "e tensor(6., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True) # we set requires_grad=True to let PyTorch know to keep the graph\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "c = a + b  # 雖沒設定 requires_grad=True, 但也有\n",
    "d = b + 1\n",
    "e = c * d\n",
    "print('c', c)\n",
    "print('d', d)\n",
    "print('e', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94745659-9296-467c-b589-6eb1e6531ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "PyTorch's f'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):  # 方程式\n",
    "    return (x-2)**2\n",
    "\n",
    "def fp(x):  # 解析解\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y = f(x)\n",
    "y.backward()  # 算微分\n",
    "\n",
    "print('Analytical f\\'(x):', fp(x))  \n",
    "print('PyTorch\\'s f\\'(x):', x.grad)  # 微分值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda53ba7-760d-453f-a502-ebcd4967a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical grad g(w) tensor([2.0000, 5.2832])\n",
      "PyTorch's grad g(w) tensor([2.0000, 5.2832])\n"
     ]
    }
   ],
   "source": [
    "def g(w):\n",
    "    return 2*w[0]*w[1] + w[1]*torch.cos(w[0])\n",
    "\n",
    "def grad_g(w):  # 解析解\n",
    "    return torch.tensor([2*w[1] - w[1]*torch.sin(w[0]), 2*w[0] + torch.cos(w[0])])\n",
    "\n",
    "w = torch.tensor([np.pi, 1], requires_grad=True)\n",
    "\n",
    "z = g(w)\n",
    "z.backward()  # 算微分  \n",
    "\n",
    "print('Analytical grad g(w)', grad_g(w))\n",
    "print('PyTorch\\'s grad g(w)', w.grad)  #微分值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edcde9f5-0adb-4356-be39-b2f1f1a03791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter,\tx,\tf(x),\tf'(x),\tf'(x) pytorch\n",
      "0,\t5.000,\t9.000,\t6.000,\t6.000\n",
      "1,\t3.500,\t2.250,\t3.000,\t3.000\n",
      "2,\t2.750,\t0.562,\t1.500,\t1.500\n",
      "3,\t2.375,\t0.141,\t0.750,\t0.750\n",
      "4,\t2.188,\t0.035,\t0.375,\t0.375\n",
      "5,\t2.094,\t0.009,\t0.188,\t0.188\n",
      "6,\t2.047,\t0.002,\t0.094,\t0.094\n",
      "7,\t2.023,\t0.001,\t0.047,\t0.047\n",
      "8,\t2.012,\t0.000,\t0.023,\t0.023\n",
      "9,\t2.006,\t0.000,\t0.012,\t0.012\n",
      "10,\t2.003,\t0.000,\t0.006,\t0.006\n",
      "11,\t2.001,\t0.000,\t0.003,\t0.003\n",
      "12,\t2.001,\t0.000,\t0.001,\t0.001\n",
      "13,\t2.000,\t0.000,\t0.001,\t0.001\n",
      "14,\t2.000,\t0.000,\t0.000,\t0.000\n"
     ]
    }
   ],
   "source": [
    "def f(x):  # 方程式\n",
    "    return (x-2)**2\n",
    "\n",
    "def fp(x):  # 解析解\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "step_size = 0.25\n",
    "\n",
    "print('iter,\\tx,\\tf(x),\\tf\\'(x),\\tf\\'(x) pytorch')\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward() # compute the gradient\n",
    "    \n",
    "    print('{},\\t{:.3f},\\t{:.3f},\\t{:.3f},\\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
    "    \n",
    "    x.data = x.data - step_size * x.grad # perform a GD update step\n",
    "    \n",
    "    # We need to zero the grad variable since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting.\n",
    "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b4ad1-01d2-4e5a-a072-b0054f477122",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20204f1c-0ac9-441d-b7a7-9b96cdc91de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor tensor([-1.,  1.,  0.])\n",
      "activated tensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "activation_fn = nn.ReLU()  # we instantiate an instance of the ReLU module\n",
    "example_tensor = torch.tensor([-1.0, 1.0, 0.0])\n",
    "activated = activation_fn(example_tensor)\n",
    "print('example_tensor', example_tensor)\n",
    "print('activated', activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5659c-9c35-4a71-bfd5-12c48bc27b07",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a41a5888-737a-45ce-b9b7-8a3ee959fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6667)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "input = torch.tensor([0., 0, 0])  # dtype: torch.float32\n",
    "target = torch.tensor([1, 0, -1])  # torch.int64\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)  # (1 + 0 + 1) / 3 = 0.6667"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a7bbc-d34c-449c-931e-d8050132fa09",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a05f44ae-5c1a-4112-9573-9290564969b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params before: Parameter containing:\n",
      "tensor([[0.7994]], requires_grad=True)\n",
      "model params after: Parameter containing:\n",
      "tensor([[0.8297]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a simple model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# create a simple dataset\n",
    "X_simple = torch.tensor([[1.]])\n",
    "y_simple = torch.tensor([[2.]])\n",
    "\n",
    "# create our optimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "y_hat = model(X_simple)\n",
    "print('model params before:', model.weight)\n",
    "loss = mse_loss_fn(y_hat, y_simple)\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()\n",
    "print('model params after:', model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205c91d-e1dc-4b02-8fc3-e6627f3fb70b",
   "metadata": {},
   "source": [
    "# torch.nn\n",
    "## 取得Module的weght和bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cd2a7dc-a2c0-4ca4-af1e-d0ff339c45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Parameter containing:\n",
      "tensor([[-0.1791, -0.2872, -0.0266],\n",
      "        [-0.3199, -0.1829,  0.5222],\n",
      "        [ 0.1710,  0.2210, -0.0474],\n",
      "        [ 0.5580,  0.1654, -0.2440]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([ 0.1062, -0.3651,  0.4903,  0.4130], requires_grad=True)\n",
      "example_tensor torch.Size([2, 3])\n",
      "transormed torch.Size([2, 4])\n",
      "\n",
      "We can see that the weights exist in the background\n",
      "\n",
      "W: Parameter containing:\n",
      "tensor([[-0.1791, -0.2872, -0.0266],\n",
      "        [-0.3199, -0.1829,  0.5222],\n",
      "        [ 0.1710,  0.2210, -0.0474],\n",
      "        [ 0.5580,  0.1654, -0.2440]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([ 0.1062, -0.3651,  0.4903,  0.4130], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_out = 4\n",
    "linear_module = nn.Linear(d_in, d_out)\n",
    "print('W:', linear_module.weight)\n",
    "print('b:', linear_module.bias)\n",
    "example_tensor = torch.tensor([[1.,2,3], [4,5,6]])\n",
    "# applys a linear transformation to the data\n",
    "transformed = linear_module(example_tensor)\n",
    "print('example_tensor', example_tensor.shape)\n",
    "print('transormed', transformed.shape)\n",
    "print()\n",
    "print('We can see that the weights exist in the background\\n')\n",
    "print('W:', linear_module.weight)\n",
    "print('b:', linear_module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9213b2c-0a37-4644-81e0-f632399fd9ef",
   "metadata": {},
   "source": [
    "## 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0814af07-5cbb-401f-ac6f-f035e95cead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hank(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class Hank(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Hank, self).__init__()\n",
    "        self.conv1 = Conv2d(3, 32, 5, padding=2)  # 可使用官方文件給的公式去求出padding\n",
    "        self.maxpool1 = MaxPool2d(2)\n",
    "        self.conv2 = Conv2d(32, 32, 5, padding=2)\n",
    "        self.maxpool2 = MaxPool2d(2)\n",
    "        self.conv3 = Conv2d(32, 64, 5, padding=2)\n",
    "        self.maxpool3 = MaxPool2d(2)\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = Linear(1024, 64)  # input數量如果算不準可以透過改forward算shape去算\n",
    "        self.linear2 = Linear(64, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "hank = Hank()\n",
    "print(hank)\n",
    "# 驗證神經網路是否可正常運行, 創建一套假數據跑看看會不會報錯\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "output = hank(input)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5945e20-2d25-47d4-94ab-d93f0c2fdf88",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b28b3cb-b297-4ad4-9664-53fd0a5b3161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hank2(\n",
      "  (model1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "\n",
    "class Hank2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Hank2, self).__init__()\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),  # 可使用官方文件給的公式去求出padding\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Sequential搭建的效果同上, 好處在於forward的時候比較簡潔\n",
    "hank2 = Hank2()\n",
    "print(hank2)\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "output = hank2(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2273992-b497-4637-9e52-ca582c20b84b",
   "metadata": {},
   "source": [
    "# 引用修改現有模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54211069-a3dd-4200-8855-d16dd5494f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torch import nn\n",
    "# # train_data = torchvision.datasets.ImageNet(\"Data/ImageNet\", split='train', download=True,\n",
    "# #                                            transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# vgg16_false = torchvision.models.vgg16(pretrained=False)\n",
    "# vgg16_true = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# # print(vgg16_true)\n",
    "\n",
    "# # train_data = torchvision.datasets.CIFAR10('Data/data', train=True, transform=torchvision.transforms.ToTensor(),\n",
    "# #                                           download=True)\n",
    "\n",
    "# vgg16_true.classifier.add_module('add_linear', nn.Linear(1000, 10))\n",
    "# # print('---'*30)\n",
    "# # print(vgg16_true)\n",
    "# # print('---'*30)\n",
    "# #\n",
    "# print(vgg16_false)\n",
    "# print('---'*30)\n",
    "# vgg16_false.classifier[6] = nn.Linear(4096, 10)\n",
    "# print(vgg16_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2530faf-920d-4c03-bcad-6758ff903ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
