{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6f3712-02d0-4465-bfa4-304915e66479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f0b58-607d-4902-b93c-343cbab3294a",
   "metadata": {},
   "source": [
    "## nn.Conv2d_set_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714b1424-5a12-4fa0-9f08-b77b6cbb5d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.shape:  torch.Size([1, 3, 3, 3])\n",
      "w1:\n",
      " Parameter containing:\n",
      "tensor([[[[ 0.0626,  0.0636, -0.1521],\n",
      "          [ 0.0695, -0.0627, -0.1351],\n",
      "          [ 0.0185, -0.0140, -0.0907]],\n",
      "\n",
      "         [[-0.0837, -0.1644, -0.0851],\n",
      "          [-0.0838, -0.1911, -0.0160],\n",
      "          [-0.0968,  0.1759, -0.0387]],\n",
      "\n",
      "         [[-0.1614, -0.0760, -0.0885],\n",
      "          [-0.0796,  0.1843, -0.0673],\n",
      "          [-0.1510, -0.0729,  0.0761]]]], requires_grad=True)\n",
      "c1.weight.shape:  torch.Size([1, 3, 3, 3])\n",
      "c1.weight:\n",
      " Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n",
      "fm:\n",
      " tensor([[[[27., 27., 27.],\n",
      "          [27., 27., 27.],\n",
      "          [27., 27., 27.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image = torch.ones((1, 3, 5, 5))\n",
    "c1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1, bias=False)\n",
    "w1 = c1.weight\n",
    "print('w1.shape: ', w1.shape)  \n",
    "print('w1:\\n', w1)\n",
    "# set weights\n",
    "c1.weight = nn.Parameter(torch.ones_like(w1), requires_grad=True)\n",
    "print('c1.weight.shape: ', c1.weight.shape)  \n",
    "print('c1.weight:\\n', c1.weight)\n",
    "\n",
    "fm = c1(image)\n",
    "print('fm:\\n', fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c67edea-e9b4-400f-98f0-845610e6b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape: torch.Size([1, 1, 5, 5])\n",
      "h_down.shape:  torch.Size([1, 2, 3, 3])\n",
      "h_up.shape:  torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn((1, 1, 5, 5))\n",
    "# print('input:\\n', input)\n",
    "print('input.shape:', input.shape)\n",
    "\n",
    "downsample = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(in_channels=2, out_channels=1, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "h_down = downsample(input)\n",
    "# print('h_down:\\n', h_down)\n",
    "print('h_down.shape: ', h_down.shape)\n",
    "\n",
    "h_up = upsample(h_down, output_size=input.size())  # 盡量要寫output_size, 確保還原尺寸\n",
    "# print('h_up:\\n', h_up)\n",
    "print('h_up.shape: ', h_up.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba419d4c-b168-4a4b-82ea-6c6a576d6e23",
   "metadata": {},
   "source": [
    "## GAP\n",
    "### 減少參數, 減少overtraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1f253e4-1992-48bc-8a82-1f9e214e7a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 5, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 5x7\n",
    "m = nn.AdaptiveAvgPool2d((5,7))\n",
    "input = torch.randn(1, 64, 8, 9)\n",
    "output = m(input)\n",
    "print(output.shape)  # torch.Size([1, 64, 5, 7])\n",
    "# target output size of 7x7 (square)\n",
    "m = nn.AdaptiveAvgPool2d(7)\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "print(output.shape)  # torch.Size([1, 64, 7, 7])\n",
    "# target output size of 10x7\n",
    "m = nn.AdaptiveAvgPool2d((None, 7))\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "print(output.shape)  # torch.Size([1, 64, 10, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f5f66-0d96-406a-aa6d-fa89e041fa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
